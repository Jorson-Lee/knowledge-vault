

# 1. 什麼是分類？

分類是一種機器學習任務，其目標是將輸入資料分配到預定義的類別中。

- **範例應用：**
    
    - **信用評分：** 根據個人的財務歷史、收入等資訊，輸出「接受」或「拒絕」。
        
    - **醫學診斷：** 根據症狀和病史，輸出「哪種疾病」。
        
    - **手寫辨識、人臉辨識：** 將圖像輸入，輸出對應的文字或人名。
        
    - **寶可夢屬性預測：** 根據寶可夢的各種能力值（總和、HP、攻擊、防禦、特攻、特防、速度），預測其屬性。
        

# 2. 分類與迴歸的比較

PDF 中提到，如果將分類問題視為迴歸問題，可能會產生問題。

- **二元分類作為迴歸的例子：**
    
    - 訓練：將類別 1 設定為目標值 1；類別 2 設定為目標值 -1。
        
    - 測試：如果預測值接近 1，則分類為類別 1；如果接近 -1，則分類為類別 2。
        
    - **問題：** 這種方法會懲罰那些「太過正確」的範例（例如，預測值為 >>1 的情況），這是不理想的。
        

# 3. 理想的替代方案

- 理想模型函數：
    
    f(x)={類別 1類別 2​如果 g(x)>0否則​
- 損失函數 (Loss Function)：
    
    L(f)=n∑​δ(f(xn)=y^​n)
    
    其中 δ(⋅) 是克羅內克函數 (Kronecker delta)，如果括號內的條件為真，則返回 1，否則返回 0。這表示損失函數是訓練資料中分類錯誤的次數。
    
- **尋找最佳函數：** 目標是找到一個函數 f 來最小化損失 L(f)。
    

# 4. 機率生成模型的核心概念

機率生成模型背後的想法是：我們不直接去尋找分類邊界，而是去模擬資料的生成過程。

- 貝氏定理 (Bayes' Theorem)：
    
    P(Ck​∣x)=P(x)P(x∣Ck​)P(Ck​)​
    
    其中：
    
    - P(Ck​∣x)：**後驗機率 (Posterior Probability)**，給定輸入 x 屬於類別 Ck​ 的機率。
        
    - P(x∣Ck​)：**類別機率 (Class-Conditional Probability)**，給定類別 Ck​ 生成輸入 x 的機率。
        
    - P(Ck​)：**先驗機率 (Prior Probability)**，在看到任何資料前，類別 Ck​ 出現的機率。
        
    - P(x)：**證據 (Evidence)** 或邊緣機率，輸入 x 出現的機率。
        
    
    對於二元分類，我們比較 P(C1​∣x) 和 P(C2​∣x) 的大小來決定分類：
    
    P(C1​∣x)=P(x∣C1​)P(C1​)+P(x∣C2​)P(C2​)P(x∣C1​)P(C1​)​

# 5. 估計機率分佈

要使用貝氏定理進行分類，我們需要從訓練資料中估計 P(Ck​) 和 P(x∣Ck​)。

- **先驗機率** P(Ck​) **的估計：**
    
    - 簡單地用訓練資料中屬於該類別的樣本數來估計。
        
    - 範例：79 個水屬性寶可夢，61 個一般屬性寶可夢。
        
        P(C1​)=P(水)=79+6179​≈0.56P(C2​)=P(一般)=79+6161​≈0.44
- **類別機率** P(x∣Ck​) **的估計：**
    
    - PDF 假設資料是從**高斯分佈 (Gaussian Distribution)** 中採樣出來的。
        
    - 高斯分佈的函數形式 (D 維度)：
        
        fμ,Σ​(x)=(2π)D/2∣Σ∣1/21​exp(−21​(x−μ)TΣ−1(x−μ))
        
        其中 μ 是均值向量 (mean vector)，Σ 是協方差矩陣 (covariance matrix)。
        
    - **最大似然法 (Maximum Likelihood Estimation, MLE)：** 用來估計最佳的 μ 和 Σ。
        
        - 似然 (Likelihood) 是指給定一組參數，觀察到這些資料的機率。
            
        - 目標是找到一組 (μ,Σ) 使得似然 L(μ,Σ) 最大化。
            
        - 對於高斯分佈，MLE 的解非常簡單：
            
            - 最佳均值 μ∗ 是樣本的平均值。
                
            - 最佳協方差矩陣 Σ∗ 是樣本的協方差矩陣。
                
                μ∗=N1​n=1∑N​xnΣ∗=N1​n=1∑N​(xn−μ∗)(xn−μ∗)T

# 6. 模型修改與決策邊界

- **假設不同的協方差矩陣 (**Σ1​=Σ2​**)：**
    
    - 根據每個類別獨立計算 μ1​,Σ1​ 和 μ2​,Σ2​。
        
    - 此時的決策邊界是**二次 (Quadratic)** 的。
        
- **假設相同的協方差矩陣 (**Σ1​=Σ2​=Σ**)：**
    
    - 參數更少，因此模型更簡潔。
        
    - 通過最大化 L(μ1​,μ2​,Σ) 來找到最佳參數。
        
    - 此時的決策邊界是**線性 (Linear)** 的。
        

# 7. 後驗機率與 Sigmoid 函數

- 後驗機率 P(C1​∣x) 可以重新表達為 Sigmoid 函數的形式：
    
    P(C1​∣x)=P(x∣C1​)P(C1​)+P(x∣C2​)P(C2​)P(x∣C1​)P(C1​)​=1+P(x∣C1​)P(C1​)P(x∣C2​)P(C2​)​1​=1+exp(−z)1​=σ(z)
    
    其中 z 是對數機率比 (log-odds)：
    
    z=lnP(x∣C2​)P(C2​)P(x∣C1​)P(C1​)​=lnP(x∣C2​)P(x∣C1​)​+lnP(C2​)P(C1​)​
- 如果我們假設 P(x∣Ck​) 是高斯分佈，並且 Σ1​=Σ2​=Σ，則 z 是一個關於 x 的線性函數。這也解釋了為什麼在協方差矩陣相同的情況下，決策邊界是線性的。
    
    z=lnP(x∣C2​)P(x∣C1​)​+lnP(C2​)P(C1​)​=ln((2π)D/2∣Σ∣1/21​exp(−21​(x−μ2​)TΣ−1(x−μ2​))(2π)D/2∣Σ∣1/21​exp(−21​(x−μ1​)TΣ−1(x−μ1​))​)+lnN2​N1​​=−21​(x−μ1​)TΣ−1(x−μ1​)+21​(x−μ2​)TΣ−1(x−μ2​)+lnN2​N1​​
    
    這個式子展開後，xTΣ−1x 項會被抵消，留下一個線性函數。