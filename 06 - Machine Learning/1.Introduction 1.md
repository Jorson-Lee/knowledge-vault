# 機器學習與深度學習簡介

## 什麼是機器學習？

- 機器學習本質上是在「尋找一個函數」(Looking for a Function)。
    
- 這個函數可以根據輸入（input）產生特定的輸出（output）。
    

### 範例：

- **語音辨識 (Speech Recognition)**：$f(聲音)="How are you"$  
    
- **圖像辨識 (Image Recognition)**：$f(圖片)="Cat" $ 
    
- **圍棋 (Playing Go)**：$f(棋盤狀態)="5-5"$(下一步棋)
    

## 不同類型的函數

### 1. **迴歸 (Regression)**

- 函數的輸出是一個**純量 (scalar)**。
    
- **範例**：根據今天的 PM2.5、氣溫等數據，預測明天的 PM2.5 值。
    

### 2. **分類 (Classification)**

- 函數的輸出是從給定的選項 (classes) 中選擇一個正確的。
    
- **範例**：垃圾郵件過濾器 $f(EMAIL)=Yes/No$。
    
- 在圍棋中，函數的輸出是 19x19 個格子中的一個，因此有 19×19 個 classes。
    

### 3. **結構化學習 (Structured Learning)**

- 函數的輸出是具有特定結構的內容，例如圖像或文件。
    
- 結構化學習可以看作是迴歸和分類的綜合應用。
    

## 如何尋找一個函數？

尋找函數的過程可以分為三個主要步驟：

### **Step 1: 定義一個帶有未知參數的函數 (Model)**

- 我們需要根據領域知識 (domain knowledge) 來定義一個函數模型。
    
- **範例**：預測 YouTube 觀看次數
    
    - 假設模型為線性函數：y=b+wx1​  
        
        - $y$: 2/26 的觀看次數 (number of views on 2/26)
            
        - $x_1$​: 2/25 的觀看次數 (number of views on 2/25)
            
        - b 和 w 是從數據中學習的未知參數，其中 w 稱為 **weight (權重)**，b 稱為 **bias (偏差)**。
            

### **Step 2: 定義損失函數 (Loss Function)**

- **損失 (Loss)** 是用來評估一組參數的好壞。
    
- **損失函數** L(b,w) 是參數的函數，用來衡量模型預測值與真實值之間的差距。
    
- **常見的損失函數**：
    
    - **均方誤差 (Mean Square Error, MSE)**：$L=\frac{1}{N}\sum\limits_{i=1}^n ​(y_i​−\hat{y_i}​)^2$
        
    - **平均絕對誤差 (Mean Absolute Error, MAE)**：$L=\frac{1}{N}\sum\limits_{i=1}^n ​|y_i​−\hat{y_i}​|$
        
    - **交叉熵 (Cross-entropy)**：當 $y$ 和 $\hat y$​ 都是機率分布時使用。
        
- 我們的目標是找到一組參數，使得損失函數的值最小。
    

### **Step 3: 最佳化 (Optimization)**

- 最佳化的目標是找到一組能使損失函數達到最小值的參數 $w^∗$ 和 $b^∗$。
    
- $w^∗,b^∗=arg\ \underset{w,b}{min}\ ​L$​ 
    
- **梯度下降 (Gradient Descent)** 是一種常見的優化方法：
    
    1. **隨機初始化**：隨機選擇一組初始參數 $w^0$ 和 $b^0$。
        
    2. **計算梯度**：計算損失函數對參數的偏微分 $\frac{∂w}{∂L}​$ 和 $\frac{∂b}{∂L}$​，這代表了在參數空間中損失函數的「坡度」。
        
    3. **更新參數**：
        
        - $w^1←w^0−\eta\frac{∂w}{∂L}​\Big|_{w=w^0}$​  
            
        - $b^1←b^0−\eta\frac{∂b}{∂L}​\Big|_{b=b^0}$
            
        - $\eta$ (eta) 稱為 **learning rate (學習率)**，是一個超參數 (hyperparameter)，控制每次更新的步長。
            
    4. **疊代更新**：重複步驟 2 和 3，直到損失函數的值不再顯著下降。
        
- **批次梯度下降 (Batch Gradient Descent)**：將訓練數據分成多個批次 (batch) 進行更新，而非一次性處理所有數據。一個 **epoch** 代表所有批次都被訓練過一次。
    

## 模型的進階應用

- **單一線性模型 (Linear models)** 存在嚴重限制，無法有效處理複雜的非線性關係。
    
- 為了解決這個問題，我們需要更靈活的模型，例如使用多個 **Sigmoid** 或 **ReLU** 函數組合。
    
- **Sigmoid** 函數可以將一個線性模型轉換為一個 S 形曲線。
    
- **ReLU (Rectified Linear Unit)** 函數則可以產生一個折線。
    
- **深度學習 (Deep Learning)** 的概念是透過堆疊多個隱藏層 (hidden layers)，形成具有特殊結構的深層網路，以處理更複雜的數據。
    

### **深度網路的運作原理**

- 深度網路的每一層都由多個神經元組成。
    
- 每個神經元會對輸入數據進行加權和 (weighted sum) 並加上偏差 (bias)，然後通過**激活函數 (Activation function)** 進行非線性轉換。
    
- 這些隱藏層的輸出會作為下一層的輸入，最終層的輸出即為模型的預測值。
    
- **範例**：
    
    - $r_i​=b_i​+\sum_j​ w_{ij}​x_j$
        
    - $a_i​=σ(r_i​)$ (其中 σ 是 Sigmoid 或 ReLU 等激活函數)
        
    - $y=b+\sum_i ​c_i​ a_i​$
        
- 整個網絡的未知參數 (包括所有的 $w,b,c$) 可以用一個向量 θ 來表示，並同樣使用梯度下降進行最佳化。

# 新模型：更多特徵 (More Features)

在機器學習中，模型的輸入特徵 (features) 對於預測的準確性至關重要。僅使用單一特徵（例如，只用「昨天觀看次數」來預測「今天觀看次數」）可能無法捕捉到所有影響結果的因素。

為了建立一個更準確的模型，我們可以納入更多的輸入特徵。

## 範例：預測 YouTube 影片觀看次數

一個更複雜的線性模型可以考慮多個影響因素，例如：

- x1​: 前一天 (yesterday) 的觀看次數
    
- x2​: 已經發佈幾天了 (how many days have passed since 1st day)
    
- x3​: 影片是否為網紅 (YouTuber) 推薦 (Is the video recommended by YouTuber?)
    
- x4​: 影片是否有出現在 CNN 新聞 (Is the video on CNN news?)
    

### 數學模型

這個新的線性模型可以表示為：

y=b+w1​x1​+w2​x2​+w3​x3​+w4​x4​  

在這個模型中，我們有多個權重 (w1​,w2​,w3​,w4​) 和一個偏差 (b) 需要從數據中學習。這些參數決定了每個特徵對於預測結果的相對重要性。

### 損失函數 (Loss Function)

我們的目標是找到一組最佳的參數 b,w1​,w2​,w3​,w4​，使得損失函數最小化。常用的損失函數，例如均方誤差 (Mean Square Error, MSE)，可以表示為：

L=∑n=1N​(yn−(b+∑i​wi​xin​))2  

這裡的 N 是數據點的總數，yn 是第 n 個數據點的真實值，xin​ 是第 n 個數據點的第 i 個特徵值。

### 最佳化 (Optimization)

與單一特徵模型類似，我們可以使用**梯度下降 (Gradient Descent)** 來最佳化這個多特徵模型。我們需要計算損失函數對每個參數的偏微分，並根據梯度方向更新參數：

wi​←wi​−η∂wi​∂L​  

b←b−η∂b∂L​  

[Inference] 這裡的 η 學習率 (learning rate) 仍然是控制更新步長的超參數。

[Inference] 儘管特徵數量增加，但優化的基本原理保持不變，我們仍然通過逐步調整參數來尋找最佳解。